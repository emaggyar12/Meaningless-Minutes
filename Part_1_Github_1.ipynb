{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: Chat GPT used for syntax and logic problems\n",
    "All credit for statistical data goes to Basketball Reference. \n",
    "Team abbreviation file created from file created by Tgemayel found on GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import certifi\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the functions used in the code\n",
    "\n",
    "# Converts starter's name into {First Initial}. {Last Name}\n",
    "def starter_conversion(starter):\n",
    "    first_name, last_name = starter.split()[:2]\n",
    "    return f\"{first_name[0]}. {last_name}\"\n",
    "\n",
    "# Finds all indices where substitutions are made and adds them to the list specified\n",
    "def sub_check(array, sub_list):\n",
    "    sub_list.extend(i for i, val in enumerate(array) if pd.notna(val) and 'enters the game for' in val)\n",
    "\n",
    "# Drops indices from play-by-play based on garbage time play\n",
    "def drop_indices(combined_list, full_list, df, offset):\n",
    "    full_list.extend(range(combined_list[0] + offset, len(df)))\n",
    "    df.drop(full_list, inplace=True)\n",
    "    print(f\"Dropped rows: {full_list}\")\n",
    "\n",
    "# Splits the score string into integer scores\n",
    "def split_score(score_string):\n",
    "    return map(int, score_string.split('-'))\n",
    "\n",
    "# Scrapes play-by-play, replacing player names with Basketball Reference codes\n",
    "def scrapegame_new(url, game_list):\n",
    "    file_name = url.split('/')[-1].replace('.html', '')\n",
    "    game_list.append(file_name)\n",
    "\n",
    "    response = requests.get(url, verify=certifi.where())\n",
    "    time.sleep(3)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Create player name to player code mapping\n",
    "    player_dict = {\n",
    "        anchor.text.strip(): anchor['href'].split('/')[-1].split('.')[0]\n",
    "        for anchor in soup.find_all('a', href=True) if '/players/' in anchor['href']\n",
    "    }\n",
    "\n",
    "    table = soup.find('table', {'id': 'pbp'})\n",
    "    if not table:\n",
    "        print(\"Play-by-play table not found.\")\n",
    "        return\n",
    "\n",
    "    # Extract table data\n",
    "    data = [\n",
    "        [\n",
    "            (cell.get_text(strip=True) if not cell.find('a', href=True) else \n",
    "             ' '.join(player_dict.get(a.text.strip(), a.text.strip()) for a in cell.find_all('a', href=True)))\n",
    "            for cell in row.find_all(['th', 'td'])\n",
    "        ]\n",
    "        for row in table.find_all('tr')\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(data).drop([0], axis=0)\n",
    "    df.drop(df.columns[[2, 4]], axis=1, inplace=True)\n",
    "\n",
    "    os.makedirs(\"games\", exist_ok=True)\n",
    "    file_path = os.path.join(\"games\", f\"{file_name}.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"Saved play-by-play data to {file_path}\")\n",
    "\n",
    "# Scrapes box score and extracts home/away teams\n",
    "def scrape_box_new(url, home_array, away_array):\n",
    "    teams = pd.read_csv('Teams.csv')\n",
    "    all_teams = teams.iloc[:, 0].tolist()\n",
    "\n",
    "    response = requests.get(url, verify=certifi.where())\n",
    "    time.sleep(3)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1').get_text(strip=True)\n",
    "    away_team, home_team = title.split(' at ')[0], title.split(' at ')[1].split('Box')[0]\n",
    "\n",
    "    away_array.append(teams.iloc[all_teams.index(away_team), 1])\n",
    "    home_array.append(teams.iloc[all_teams.index(home_team), 1])\n",
    "\n",
    "    df_list = pd.read_html(response.text)\n",
    "    away_df, home_df = df_list[0], df_list[8]\n",
    "\n",
    "    folder_path = os.path.join(\"games\", url.split('/')[-1].replace('.html', ''))\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    away_path = os.path.join(folder_path, \"away_box.csv\")\n",
    "    home_path = os.path.join(folder_path, \"home_box.csv\")\n",
    "    away_df.to_csv(away_path, index=False)\n",
    "    home_df.to_csv(home_path, index=False)\n",
    "    \n",
    "    print(f\"Saved box scores: {away_path}, {home_path}\")\n",
    "\n",
    "# Function to analyze winning margin and garbage time plays\n",
    "def analyze_winning_margin(df):\n",
    "    df['time_seconds'] = df.iloc[:, 0].apply(lambda x: int(x.split(':')[0]) * 60 + int(x.split(':')[1]) if ':' in str(x) else x)\n",
    "    df = df.sort_values(by='time_seconds', ascending=False)\n",
    "    \n",
    "    drop_rows = []\n",
    "    for i in range(len(df) - 1):\n",
    "        curr_score = split_score(df.iloc[i, 2])\n",
    "        next_score = split_score(df.iloc[i + 1, 2])\n",
    "        if abs(curr_score[0] - curr_score[1]) >= 4 and abs(next_score[0] - next_score[1]) >= 4:\n",
    "            drop_rows.append(i)\n",
    "    \n",
    "    df.drop(drop_rows, inplace=True)\n",
    "    print(f\"Dropped garbage time rows: {drop_rows}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs code for example games\n",
    "all_true_names_list = pd.read_csv(\"Players_True.csv\")[\"Player\"].astype(str).tolist()\n",
    "\n",
    "# URL of the page to scrape\n",
    "box_scores = [\n",
    "    \"https://www.basketball-reference.com/boxscores/202310250CHO.html\",\n",
    "    \"https://www.basketball-reference.com/boxscores/202404230LAC.html\",\n",
    "    \"https://www.basketball-reference.com/boxscores/202403030BOS.html\",\n",
    "    \"https://www.basketball-reference.com/boxscores/202404030MIN.html\"\n",
    "]\n",
    "\n",
    "play_by_play_urls = [url.replace('/boxscores/', '/boxscores/pbp/') for url in box_scores]\n",
    "\n",
    "game_names, home_teams, away_teams = [], [], []\n",
    "\n",
    "for url in play_by_play_urls:\n",
    "    scrapegame_new(url, game_names)\n",
    "for url in box_scores:\n",
    "    scrape_box_new(url, home_teams, away_teams)\n",
    "\n",
    "# Process winning margin analysis\n",
    "for game_name in game_names:\n",
    "    play_by_play_file = os.path.join(\"games\", f\"{game_name}.csv\")\n",
    "    df = pd.read_csv(play_by_play_file)\n",
    "    df = analyze_winning_margin(df)\n",
    "    df.to_csv(play_by_play_file, index=False)\n",
    "    print(f\"Processed and saved adjusted play-by-play data for {game_name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
